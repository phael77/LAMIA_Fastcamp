{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (from groq) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (from groq) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /home/phael/LAMIA_Fastcamp/.venv/lib/python3.12/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Instalação de pacotes necessários para execução do código\n",
    "%pip install groq\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv #biblioteca para carregar variáveis de ambiente\n",
    "from groq import Groq #biblioteca que possibilita o uso da api do groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acessando variáveis de ambiente, uma delas é a chave da api do Groq\n",
    "load_dotenv()\n",
    "\n",
    "#Para não precisar ficar passando a chave da api toda vez que for instanciar um objeto Groq,\n",
    "#E também não expor a chave da api, ela foi salva em uma variável de ambiente\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTE DA API DO GROQ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in modern natural language processing (NLP) and have numerous applications in various industries. Here are some reasons why fast language models are important:\n",
      "\n",
      "1. **Real-time Processing**: Fast language models enable real-time processing of large amounts of text data, which is essential for applications like chatbots, virtual assistants, and sentiment analysis. Quick processing times ensure that responses are generated swiftly, providing a seamless user experience.\n",
      "2. **Scalability**: Fast language models can handle large volumes of data, making them suitable for big data applications, such as analyzing social media posts, customer feedback, or vast amounts of text data generated by IoT devices.\n",
      "3. **Efficient Resource Utilization**: Fast language models reduce the computational resources required to process language-related tasks, leading to cost savings and more efficient use of hardware.\n",
      "4. **Improved User Experience**: Fast language models enable quick response times, which is critical in applications like language translation, spell checking, and text summarization. This results in a better user experience, increased productivity, and improved customer satisfaction.\n",
      "5. **Enhanced Accuracy**: Fast language models can process large amounts of data quickly, allowing for more accurate modeling of language patterns, which leads to better performance in NLP tasks like language translation, question answering, and text classification.\n",
      "6. **Edge AI and IoT Applications**: Fast language models are essential for edge AI and IoT applications, where devices have limited processing power and memory. Fast models enable real-time processing and decision-making at the edge, reducing latency and improving performance.\n",
      "7. **Low-Latency Applications**: Fast language models are critical in applications that require ultra-low latency, such as autonomous vehicles, medical diagnosis, and real-time decision-making systems.\n",
      "8. **Competitive Advantage**: In industries like customer service, finance, and healthcare, fast language models can provide a competitive advantage by enabling faster response times, improved accuracy, and enhanced user experience.\n",
      "9. **Accessibility**: Fast language models can enable language accessibility features, such as real-time translation, for people with disabilities, improving their quality of life and participation in society.\n",
      "10. **Research and Development**: Fast language models accelerate research and development in NLP, enabling scientists and engineers to explore new ideas, test hypotheses, and develop innovative applications more quickly.\n",
      "11. **Environmental Sustainability**: By reducing the computational resources required for language processing, fast language models can contribute to environmental sustainability by minimizing energy consumption and e-waste generation.\n",
      "12. **Cybersecurity**: Fast language models can help detect and respond to cyber threats more quickly, reducing the risk of data breaches and improving overall cybersecurity.\n",
      "\n",
      "In summary, fast language models are essential for various applications that require rapid processing, accuracy, and scalability. They have the potential to transform industries, improve user experiences, and contribute to environmental sustainability and social good.\n"
     ]
    }
   ],
   "source": [
    "#Aqui instanciamos um objeto Groq, passando a chave da api como parâmetro\n",
    "client = Groq(\n",
    "    api_key = groq_api_key,\n",
    ")\n",
    "\n",
    "#Aqui é feita a requisição para o modelo de linguagem, passando uma mensagem de exemplo\n",
    "#E o modelo de linguagem a ser utilizado\n",
    "chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": \"Explain the importance of fast language models\",\n",
    "            }\n",
    "    ],\n",
    "    model = \"llama3-70b-8192\"\n",
    ")\n",
    "\n",
    "#Aqui é impresso a resposta do modelo de linguagem\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEFINIÇÃO DA CLASSE AGENTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, client, system): #Inicialização do nosso objeto Agent, feito pelo construtor\n",
    "        self.client = client #Atributo que guarda o cliente da api do Groq\n",
    "        self.system = system #Atributo que guarda a mensagem de sistema\n",
    "        self.messages = [] #Atributo que guarda as mensagens trocadas entre o usuário e o assistente\n",
    "        #Se a mensagem de sistema não for nula, ela é adicionada ao atributo messages\n",
    "        if self.system is not None:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": self.system}) \n",
    "        \n",
    "    def __call__(self, message): #Método que é chamado quando o objeto é instanciado\n",
    "        #Se a mensagem não for nula, ela é adicionada ao atributo messages\n",
    "        if message:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute() #O método execute é chamado e o resultado é guardado em uma variável\n",
    "        self.messsages.append({\"role\": \"assistant\", \"content\": result}) #A resposta do modelo de linguagem é adicionada ao atributo messages\n",
    "        return result #Resultado é retornado\n",
    "    \n",
    "    def execute(self): #Método que executa a requisição para o modelo de linguagem\n",
    "        completion = client.chat.completions.create(\n",
    "                    messages=self.messages,\n",
    "                    model = \"llama3-70b-8192\",\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
